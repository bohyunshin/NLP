{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python 3.6에서 glove 임베딩 벡터 만들어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* lovit님의 glove 포스팅\n",
    "* genism의 glove python library\n",
    "* soynlp의 noun extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 데이터\n",
    "* 2016년 10월 20일 뉴스기사 (lovit님의 튜토리얼 데이터)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NewsNounExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "corpus_fname = '2016-10-20_article_all_normed.txt'\n",
    "sentences = DoublespaceLineCorpus(corpus_fname, iter_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used default noun predictor; Sejong corpus based logistic predictor\n",
      "/Users/shinbo/opt/anaconda3/envs/glove/lib/python3.6/site-packages/soynlp\n",
      "local variable 'f' referenced before assignment\n",
      "local variable 'f' referenced before assignment\n",
      "scan vocabulary ... \n",
      "done (Lset, Rset, Eojeol) = (658116, 363342, 403882)\n",
      "predicting noun score was done                                        \n",
      "before postprocessing 237871\n",
      "_noun_scores_ 50196\n",
      "checking hardrules ... done0 / 50196+(이)), NVsubE (사기(당)+했다) ... done\n",
      "after postprocessing 36027\n",
      "extracted 2365 compounds from eojeolss ... 87000 / 87714"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import NewsNounExtractor\n",
    "\n",
    "noun_extractor = NewsNounExtractor(\n",
    "    max_left_length=10, \n",
    "    max_right_length=7,\n",
    "    predictor_fnames=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nouns = noun_extractor.train_extract(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근혜: (score=0.478, frequency=1507)\n",
      "우병우: (score=0.757, frequency=721)\n",
      "민정수석: (score=0.834, frequency=812)\n",
      "아이오아이: (score=0.547, frequency=270)\n",
      "최순실: (score=0.828, frequency=1878)\n",
      "게이트: (score=0.745, frequency=307)\n",
      "콘서트: (score=0.769, frequency=500)\n"
     ]
    }
   ],
   "source": [
    "words = ['박근혜', '우병우', '민정수석', \n",
    "         '트와이스', '아이오아이', '최순실',\n",
    "         '최순실게이트', '게이트', '콘서트']\n",
    "\n",
    "for word in words:\n",
    "    if not word in nouns:\n",
    "        continue\n",
    "    score = nouns[word]\n",
    "    print('%s: (score=%.3f, frequency=%d)' \n",
    "          % (word, score.score, score.frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223357/223357 [57:46<00:00, 64.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "nouns_list = list(nouns.keys())\n",
    "with open('2016-10-20_noun.txt','w') as f:\n",
    "    \n",
    "    for sent in tqdm(sentences):\n",
    "        split_words = sent.split(' ')\n",
    "        sent_noun_only = [word for word in split_words if word in nouns_list]\n",
    "        f.write(' '.join(sent_noun_only) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove: unpreprocessed data vs only noun data\n",
    "\n",
    "### data\n",
    "* 2016-10-20.txt: unpreprocessed data\n",
    "* 2016-10-20_noun.txt: noun extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for unpreprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 223356 sents, mem=1.156 Gb\n",
      "  - scanning (word, context) pairs from 223356 sents, mem=1.378 Gb\n",
      "  - (word, context) matrix was constructed. shape = (50091, 50091)                    \n",
      "  - done\n",
      "(50091, 50091)\n",
      "Performing 5 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "\n",
      "아이오아이\n",
      "('신용재', 0.9051910042145265)\n",
      "('세븐', 0.8470469539504547)\n",
      "('남남서쪽', 0.8274233617915241)\n",
      "('경주시', 0.8070812919370273)\n",
      "('수원', 0.8025955895553446)\n",
      "('노선이', 0.7974367242614192)\n",
      "('타운점', 0.7972171103611738)\n",
      "('배추가', 0.7943929049352353)\n",
      "('120', 0.7886975207427144)\n",
      "\n",
      "\n",
      "\n",
      "아프리카\n",
      "('쪽지', 0.7329325267092706)\n",
      "('볼트', 0.7191221188708016)\n",
      "('85', 0.7176457521023469)\n",
      "('35', 0.7135558225150953)\n",
      "('북위', 0.7122671306744952)\n",
      "('쿵푸팬더3', 0.7083750171571145)\n",
      "('자매', 0.6957292799382121)\n",
      "('엑스레이인', 0.6936836999836651)\n",
      "('김구라', 0.6920186556116092)\n",
      "\n",
      "\n",
      "\n",
      "박근혜\n",
      "('역적패당의', 0.8990734106563494)\n",
      "('가소로운', 0.8957872330269615)\n",
      "('대통령의', 0.8673564054807855)\n",
      "('주체위성들은', 0.8577487264303695)\n",
      "('끝장내자', 0.8513777109891596)\n",
      "('방북', 0.8407514214149523)\n",
      "('대통령이', 0.7709791371175293)\n",
      "('연설문을', 0.7692059526228373)\n",
      "('고치기', 0.7677365356464839)\n",
      "\n",
      "\n",
      "\n",
      "뉴스\n",
      "('기다립니다', 0.8961867412945723)\n",
      "('서비스입니다', 0.8866714224537888)\n",
      "('리얼타임', 0.8681836138804334)\n",
      "('미란다', 0.8608811394712915)\n",
      "('제보를', 0.85910288742897)\n",
      "('머니', 0.8425699886252014)\n",
      "('가치나', 0.8418043327071293)\n",
      "('김병준', 0.8362408801117358)\n",
      "('소중한', 0.8301797872829616)\n",
      "\n",
      "\n",
      "\n",
      "날씨\n",
      "('시중은행이', 0.6890827053255749)\n",
      "('가동을', 0.6866869302576069)\n",
      "('생명과학부', 0.6740392321002497)\n",
      "('하셨는데', 0.6706417859803011)\n",
      "('등과의', 0.6698300758441451)\n",
      "('저금리가', 0.6676371935967854)\n",
      "('논의는', 0.6663213653531417)\n",
      "('스펙트럼으로', 0.6610034305371495)\n",
      "('도우미', 0.6578801808908838)\n",
      "\n",
      "\n",
      "\n",
      "이화여대\n",
      "('특혜입학', 0.781743598155208)\n",
      "('총장이', 0.7545994249454144)\n",
      "('아트하우스', 0.7372757377500538)\n",
      "('교수협의회', 0.7297493082473621)\n",
      "('사퇴한', 0.7258446508085239)\n",
      "('최경희', 0.7196096336361963)\n",
      "('골목에', 0.7172024492132935)\n",
      "('서대문구', 0.7151843711566155)\n",
      "('모모영화관에서', 0.7147384802091942)\n",
      "\n",
      "\n",
      "\n",
      "아프리카발톱개구리\n",
      "('일환이다', 0.850551795256279)\n",
      "('이광수', 0.8380401789827949)\n",
      "('지지율을', 0.8380056658988787)\n",
      "('등장을', 0.8349418715855446)\n",
      "('협동', 0.8291829787544097)\n",
      "('재연해', 0.8185455819929336)\n",
      "('디올', 0.8178690808650093)\n",
      "('동화같은', 0.8143335998545738)\n",
      "('윤상현', 0.8125235496681544)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "corpus_path = \"2016-10-20.txt\"\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "\n",
    "# make corpus to cooccurrence matrix to input to the glove\n",
    "# window is 3\n",
    "x, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows=3,\n",
    "    min_tf=10,\n",
    "    tokenizer=lambda x:x.split(), # (default) lambda x:x.split(),\n",
    "    dynamic_weight=True,\n",
    "    verbose=True)\n",
    "\n",
    "print(x.shape) # total 50091 words\n",
    "\n",
    "# glove library from genism\n",
    "from glove import Glove\n",
    "glove = Glove(no_components=100, learning_rate=0.05, max_count=30)\n",
    "glove.fit(x.tocoo(), epochs=5, no_threads=4, verbose=True)\n",
    "\n",
    "dictionary = {vocab:idx for idx, vocab in enumerate(idx2vocab)}\n",
    "glove.add_dictionary(dictionary)\n",
    "\n",
    "words = '아이오아이 아프리카 박근혜 뉴스 날씨 이화여대 아프리카발톱개구리'.split()\n",
    "for word in words:\n",
    "    print('\\n{}'.format(word))\n",
    "    for tup in glove.most_similar(word, number=10):\n",
    "        print(tup)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for noun-extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 204055 sents, mem=1.276 Gb\n",
      "  - scanning (word, context) pairs from 204055 sents, mem=1.286 Gb\n",
      "  - (word, context) matrix was constructed. shape = (10328, 10328)                    \n",
      "  - done\n",
      "Performing 5 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "\n",
      "아이오아이\n",
      "('세븐', 0.9192878295544598)\n",
      "('코드', 0.829506920496379)\n",
      "('불독', 0.8231956126330225)\n",
      "('에이핑크', 0.8145294353687739)\n",
      "('산들', 0.8084104194412042)\n",
      "('멤버들', 0.7959946220913665)\n",
      "('걸그룹', 0.7828779355233834)\n",
      "('소라', 0.7642841465878357)\n",
      "('합정동', 0.7597687792163058)\n",
      "\n",
      "\n",
      "\n",
      "아프리카\n",
      "('4대', 0.6648020293131455)\n",
      "('원조', 0.6578438627359317)\n",
      "('식음료', 0.6535012684994946)\n",
      "('3년', 0.6474019387268591)\n",
      "('2000년대', 0.646889113704001)\n",
      "('상주', 0.6373969293981944)\n",
      "('전량', 0.6199516493057061)\n",
      "('연속', 0.6198023174595333)\n",
      "('식료품', 0.6124910881474588)\n",
      "\n",
      "\n",
      "\n",
      "박근혜\n",
      "('김정일', 0.8291159560711128)\n",
      "('고치기', 0.8184660762511573)\n",
      "('정권', 0.8119924380355885)\n",
      "('방북', 0.7926370300459487)\n",
      "('대책위원회', 0.7795758246093294)\n",
      "('미르', 0.7679082116618466)\n",
      "('최순실', 0.7659296482412842)\n",
      "('비선실세', 0.7607943657820737)\n",
      "('편파기소', 0.7592796050358387)\n",
      "\n",
      "\n",
      "\n",
      "뉴스\n",
      "('독자', 0.9534583975283663)\n",
      "('영상', 0.9504055118635136)\n",
      "('튜브', 0.945729554916657)\n",
      "('속보', 0.9340251866959389)\n",
      "('미디어오늘', 0.8998197695472183)\n",
      "('머니투데이', 0.8993230761359143)\n",
      "('스타뉴스', 0.8985782785588267)\n",
      "('대학경제', 0.8981522830119457)\n",
      "('김은혜', 0.8926088879247833)\n",
      "\n",
      "\n",
      "\n",
      "날씨\n",
      "('부위', 0.824357001216767)\n",
      "('2부', 0.8126870880566979)\n",
      "('인터파크', 0.807412005953926)\n",
      "('잃게', 0.7988465524280459)\n",
      "('휴일', 0.7953510471479075)\n",
      "('공모전', 0.7952965935412043)\n",
      "('휴전', 0.7941899099199226)\n",
      "('세종문화회관', 0.7927316675892727)\n",
      "('서울대병원', 0.7807786661051382)\n",
      "\n",
      "\n",
      "\n",
      "이화여대\n",
      "('최경희', 0.9201083588679363)\n",
      "('이대', 0.901106453040368)\n",
      "('사퇴', 0.877326728116605)\n",
      "('특혜', 0.8753341237954702)\n",
      "('특혜입학', 0.8639351257922703)\n",
      "('입학', 0.8372312694589804)\n",
      "('총장', 0.8338870392533818)\n",
      "('정유라', 0.787791738171998)\n",
      "('고영태', 0.7576258669971372)\n",
      "\n",
      "\n",
      "\n",
      "아프리카발톱개구리\n",
      "('유전체', 0.7824479627028293)\n",
      "('일정', 0.7808454381027139)\n",
      "('90분', 0.7743494068361466)\n",
      "('그랬듯', 0.7719074235474841)\n",
      "('며칠', 0.7679344366015137)\n",
      "('27년', 0.7580311303733899)\n",
      "('3개월', 0.7567721799566873)\n",
      "('동물실험', 0.7510106223975127)\n",
      "('자동화기기', 0.7508184565745522)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "corpus_path = \"2016-10-20_noun.txt\"\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "\n",
    "# make corpus to cooccurrence matrix to input to the glove\n",
    "# window is 3\n",
    "x, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows=3,\n",
    "    min_tf=10,\n",
    "    tokenizer=lambda x:x.split(), # (default) lambda x:x.split(),\n",
    "    dynamic_weight=True,\n",
    "    verbose=True)\n",
    "\n",
    "x.shape\n",
    "\n",
    "# glove library from genism\n",
    "from glove import Glove\n",
    "glove = Glove(no_components=100, learning_rate=0.05, max_count=30)\n",
    "glove.fit(x.tocoo(), epochs=5, no_threads=4, verbose=True)\n",
    "\n",
    "dictionary = {vocab:idx for idx, vocab in enumerate(idx2vocab)}\n",
    "glove.add_dictionary(dictionary)\n",
    "\n",
    "words = '아이오아이 아프리카 박근혜 뉴스 날씨 이화여대 아프리카발톱개구리'.split()\n",
    "for word in words:\n",
    "    print('\\n{}'.format(word))\n",
    "    for tup in glove.most_similar(word, number=10):\n",
    "        print(tup)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과: 모든 pos vs 명사만 추출\n",
    "* 명사만 추출한 corpus의 결과가 조금 더 낫다.\n",
    "* 아이오아이 단어와의 상위 10개 단어 중, 멤버들, 걸그룹, 에이핑크 등이 추가되었다.\n",
    "* 또한 아프리카 단어와의 상위 10개 단어에서도 원조, 식음료 등의 단어가 등장하였다.\n",
    "* 아프리카발톱개구리 같은 합성어에 대해서, 특히 명사로만 이루어진 corpus의 co-occurr mat을 입력으로 했을 때, 좋은 결과를 보였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another work\n",
    "* glove는 입력으로 co-occurr을 받는다. 하지만 이를 바꿀 수도 있다. 예를 들어 PPMI를 사용할 수도 있는데, PPMI를 통해서 문맥이 뛰어난 단어들을 골라낼 수 있기 때문이다.\n",
    "* 아래와 같이 ppmi 행렬을 만들 수 있는데, 다양한 선택지 중 하나로 남겨두자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "from soynlp.word import pmi\n",
    "\n",
    "corpus_path = '2016-10-20_article_all_normed_ltokenize.txt'\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "\n",
    "x, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows=3,\n",
    "    min_tf=10,\n",
    "    tokenizer=lambda x:x.split(), # (default) lambda x:x.split(),\n",
    "    dynamic_weight=True,\n",
    "    verbose=True)\n",
    "\n",
    "pmi_dok = pmi(\n",
    "    x,\n",
    "    min_pmi=0,\n",
    "    alpha=0.0001,\n",
    "    verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
