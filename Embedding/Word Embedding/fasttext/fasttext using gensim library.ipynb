{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* 본 자료는 lovit님의 fasttext tutorial 자료를 따라하며 공부한 것임을 밝혀둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Up\n",
    "* windows에서 fasttext build up은 https://medium.com/@juneoh/windows%EC%9A%A9-fasttext-%EB%B0%94%EC%9D%B4%EB%84%88%EB%A6%AC%EB%93%A4-727829b010a를 참조하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasttext를 학습하기 위해서는 두 가지 방법이 있습니다.<br>\n",
    "* gensim에서 제공하는 FastText 이용하기\n",
    "* Facebook에서 제공하는 모듈 사용하기<br>\n",
    "\n",
    "이번 tutorial에서는 gensim에서 제공하는 FastText를 이용하여 학습을 해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글에 fasttext를 적용하려면 초/중/종성을 분리해야 한다. 따라서 이를 수행하는 함수를 먼저 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config\n",
    "from soynlp.hangle import decompose, compose\n",
    "from soynlp.normalizer import remove_doublespace\n",
    "\n",
    "## lovit님의 function\n",
    "\n",
    "def encode(s):\n",
    "    def process(c):\n",
    "        if c == ' ':\n",
    "            return c\n",
    "        jamo = decompose(c)\n",
    "        # 'a' or 모음 or 자음\n",
    "        if (jamo is None) or (jamo[0] == ' ') or (jamo[1] == ' '):\n",
    "            return ' '\n",
    "        base = jamo[0]+jamo[1]\n",
    "        if jamo[2] == ' ':\n",
    "            return base + '-'\n",
    "        return base + jamo[2]\n",
    "\n",
    "    s = ''.join(process(c) for c in s)\n",
    "    return remove_doublespace(s).strip()\n",
    "\n",
    "def decode(s):\n",
    "    def process(t):\n",
    "        assert len(t) % 3 == 0\n",
    "        t_ = t.replace('-', ' ')\n",
    "        chars = [tuple(t_[3*i:3*(i+1)]) for i in range(len(t_)//3)]\n",
    "        recovered = [compose(*char) for char in chars]\n",
    "        recovered = ''.join(recovered)\n",
    "        return recovered\n",
    "\n",
    "    return ' '.join(process(t) for t in s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅈㅏ-ㅇㅕㄴㅇㅓ-ㅊㅓ-ㄹㅣ-ㄴㅡㄴ ㅇㅓ-ㄹㅕㅂㄱㅣ-ㄷㅗ- ㅎㅏ-ㄱㅗ- ㅈㅐ-ㅁㅣ-ㅇㅣㅆㅇㅓ-ㅇㅛ-\n",
      "자연어처리는 어렵기도 하고 재미있어요\n"
     ]
    }
   ],
   "source": [
    "print(encode('자연어처리는 어렵기도 하고 재미있어요'))\n",
    "print(decode(encode('자연어처리는 어렵기도 하고 재미있어요')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasttext의 input은 word2vec과 유사한 형태로, list안에 각 문서(리뷰)별로 list가 있으며, 리뷰들이 tokenize된 형태이다.<br>\n",
    "하지만 word2vec과 다른점은 초/중/종성으로 tokenize된 형태라는 것이다.<br>\n",
    "따라서 네이버 영화 리뷰 데이터를 아래와 같이 초중종성을 분리해서 fasttext input으로 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/sbh0613/Desktop/NLP/dataset/ratings_train.txt'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "naver_comments = [i.split('\\t')[1] for i in lines][1:]\n",
    "\n",
    "fasttext_corpus = [ encode(i).split(' ') for i in naver_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('C:/Users/sbh0613/Desktop/NLP/dataset/ratings_train_preprocessed_fasttext.txt','wb') as f:\n",
    "    pickle.dump(fasttext_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(\n",
    "        fasttext_corpus,\n",
    "        window = 3,\n",
    "        min_count = 10,\n",
    "        min_n = 3,\n",
    "        max_n = 6\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim의 fasttext의 model은 word2vec의 model 사용법과 유사하다. 즉, word vector을 뽑으려면 .wv를 해야한다는 점, 그리고 가장 비슷한 단어(fasttext에서는 초/중/종성이 분리된 단어)도 .most_similar을 통해 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasttext의 input으로 추/중/종성으로 분해된 단어를 입력했으니, fasttext model에 학습된 단어도 초/중/종성이 분해된 형태이다. 그 형태는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅇㅏ-',\n",
       " 'ㄷㅓ-ㅂㅣㅇ',\n",
       " 'ㅈㅣㄴㅉㅏ-',\n",
       " 'ㅉㅏ-ㅈㅡㅇㄴㅏ-ㄴㅔ-ㅇㅛ-',\n",
       " 'ㅁㅗㄱㅅㅗ-ㄹㅣ-',\n",
       " 'ㅎㅡㅁ',\n",
       " 'ㅍㅗ-ㅅㅡ-ㅌㅓ-ㅂㅗ-ㄱㅗ-',\n",
       " 'ㄱㅏ-ㅂㅕㅂㅈㅣ-',\n",
       " 'ㅅㅗㄹㅈㅣㄱㅎㅣ-',\n",
       " 'ㅈㅐ-ㅁㅣ-ㄴㅡㄴ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fasttext_model.wv.vocab.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초/중/종성이 분리된 형태가 아니라 우리가 익숙한 단어의 형태로 바꿔주는 형태로 복원해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_agg(fasttext_model, hangle, topn = 10):\n",
    "    hangle = encode(hangle)\n",
    "    most_sim = fasttext_model.wv.most_similar(hangle, topn = topn)\n",
    "    most_sim_agg = [(decode(word), sim)  for word, sim in most_sim]\n",
    "    return most_sim_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영활', 0.9968951940536499),\n",
       " ('영환', 0.9958025217056274),\n",
       " ('영화계', 0.9909502863883972),\n",
       " ('영화평', 0.9877427220344543),\n",
       " ('영화속', 0.9873830080032349),\n",
       " ('영화판', 0.9871527552604675),\n",
       " ('영화군', 0.9871389269828796),\n",
       " ('영화란', 0.9855873584747314),\n",
       " ('영화화', 0.9830932021141052),\n",
       " ('영화랑', 0.9801906943321228)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_agg(fasttext_model, '영홯', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('쓰렉', 0.9919682741165161),\n",
       " ('쓰레기네', 0.9732194542884827),\n",
       " ('쓰레기야', 0.9635607004165649),\n",
       " ('쓰레기라', 0.9611065983772278),\n",
       " ('쓰레기', 0.9581952095031738),\n",
       " ('쓰레기임', 0.9467074871063232),\n",
       " ('쓰레기로', 0.9234054684638977),\n",
       " ('쓰레기인', 0.9211544990539551),\n",
       " ('쓰레기들', 0.9206268191337585),\n",
       " ('쓰레기통에', 0.8906248211860657)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_agg(fasttext_model, '쓰뤠', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
