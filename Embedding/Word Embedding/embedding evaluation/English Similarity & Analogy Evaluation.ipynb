{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding evaluation in English\n",
    "\n",
    "### Reference\n",
    "* evaluation methods for unsupervised word emeddings, Tobias Schnabel et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word similarity test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list of datasets\n",
    "* wordsim353\n",
    "* MEN: MEN1과 MEN2는 동일한데 MEN2가 lemmatization을 한 것 같다. 따라서 MEN2를 사용한다.\n",
    "* turk\n",
    "* rare_words\n",
    "* simlex-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_rel = pd.read_csv('./wordsim353/wordsim_relatedness_goldstandard.txt', sep='\\t', header=None)\n",
    "wordsim_sim = pd.read_csv('./wordsim353/wordsim_similarity_goldstandard.txt', sep='\\t', header=None)\n",
    "MEN1 = pd.read_csv('./MEN/elias-men-ratings.csv')\n",
    "MEN2 = pd.read_csv('./MEN/marcos-men-ratings.csv')\n",
    "turk = pd.read_csv('./Mtruk.csv', header=None)\n",
    "rare_words = pd.read_csv('./rare_words/rw.txt', sep='\\t', header=None)\n",
    "rare_words = rare_words.iloc[:,[0,1,2]]\n",
    "simlex = pd.read_csv('./SimLex-999/SimLex-999.txt', sep='\\t')\n",
    "simlex = simlex[['word1','word2','SimLex999']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_list = [wordsim_rel, wordsim_sim, MEN2, turk, rare_words, simlex]\n",
    "sim_df_list_name = ['wordsim_rel', 'wordsim_sim', 'MEN2', 'turk', 'rare_words', 'simlex']\n",
    "\n",
    "for df in sim_df_list:\n",
    "    df.columns = ['word1','word2','score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load google's pretrained word2vec model\n",
    "* 300 차원 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sbh0613\\\\Desktop\\\\NLP\\\\embedding part\\\\embedding evaluation'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "file_name = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(fname = file_name, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbh0613\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# get word vectors\n",
    "word_vectors = model.wv\n",
    "\n",
    "# get vocabulary\n",
    "vocabs = word_vectors.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr(df, model):\n",
    "    '''\n",
    "    input\n",
    "    1. df: name of dataframe in which there is human-score\n",
    "    The column of df should be word1, word2, score\n",
    "    \n",
    "    2. word_set: tuple of two words\n",
    "    \n",
    "    output: correlation between cosine similarity between two word vectors and human-score\n",
    "    '''\n",
    "    \n",
    "    n = df.shape[0]\n",
    "    \n",
    "    eval_word_list = []\n",
    "    for tup in zip(df['word1'], df['word2']):\n",
    "        eval_word_list.append(set(tup))\n",
    "    \n",
    "    word_vectors = model.wv\n",
    "    vocabs_w2v = list(word_vectors.vocab.keys())\n",
    "    \n",
    "    cosine_sim = []\n",
    "    \n",
    "    word_set_idx = 0\n",
    "    word_set_idx_list = []\n",
    "    for tup in zip(df['word1'], df['word2']):\n",
    "        if tup[0] in vocabs_w2v and tup[1] in vocabs_w2v:\n",
    "            cosine_sim.append(word_vectors.similarity(w1 = tup[0], w2 = tup[1]))\n",
    "            word_set_idx_list.append(word_set_idx)\n",
    "            word_set_idx += 1\n",
    "            \n",
    "    score = [j for idx, j in enumerate(df['score']) if idx in word_set_idx_list]\n",
    "    \n",
    "    r = np.corrcoef(score, cosine_sim)\n",
    "    \n",
    "    print('전체 {0}개 중에 {1}개가 평가로 사용됨'.format(n, len(word_set_idx_list)))\n",
    "    \n",
    "    return r[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 252개 중에 252개가 평가로 사용됨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbh0613\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5920509855347379"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_corr(wordsim_rel, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbh0613\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 252개 중에 252개가 평가로 사용됨\n",
      "dataset: wordsim_rel\n",
      "상관계수: 0.5920509855347379\n",
      "----------------------------------------------------------------\n",
      "전체 203개 중에 203개가 평가로 사용됨\n",
      "dataset: wordsim_rel\n",
      "상관계수: 0.7645224545856311\n",
      "----------------------------------------------------------------\n",
      "전체 3000개 중에 2946개가 평가로 사용됨\n",
      "dataset: wordsim_rel\n",
      "상관계수: 0.04935611822850241\n",
      "----------------------------------------------------------------\n",
      "전체 287개 중에 275개가 평가로 사용됨\n",
      "dataset: wordsim_rel\n",
      "상관계수: 0.03407367309624036\n",
      "----------------------------------------------------------------\n",
      "전체 2034개 중에 1825개가 평가로 사용됨\n",
      "dataset: wordsim_rel\n",
      "상관계수: -0.012723297635601458\n",
      "----------------------------------------------------------------\n",
      "전체 999개 중에 999개가 평가로 사용됨\n",
      "dataset: wordsim_rel\n",
      "상관계수: 0.45392820971322645\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for df in sim_df_list:\n",
    "    r = compute_corr(df, model)\n",
    "    print('dataset: {0}'.format(sim_df_list_name[0]))\n",
    "    print('상관계수: {0}'.format(r))\n",
    "    print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word analogy test\n",
    "* dataset link: https://aclweb.org/aclwiki/Analogy_(State_of_the_art)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list of datasets\n",
    "* MSR dataset (찾치 못함)\n",
    "* Google analogy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method\n",
    "* 3CosAdd\n",
    "* 3CosMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = zip(*((map(str, line.split()) for line in open('google_analogy.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_dataset = []\n",
    "for tup in zip(a,b,c,d):\n",
    "    analogy_dataset.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', 'Greece', 'Baghdad', 'Iraq'),\n",
       " ('Athens', 'Greece', 'Bangkok', 'Thailand'),\n",
       " ('Athens', 'Greece', 'Beijing', 'China'),\n",
       " ('Athens', 'Greece', 'Berlin', 'Germany'),\n",
       " ('Athens', 'Greece', 'Bern', 'Switzerland'),\n",
       " ('Athens', 'Greece', 'Cairo', 'Egypt'),\n",
       " ('Athens', 'Greece', 'Canberra', 'Australia'),\n",
       " ('Athens', 'Greece', 'Hanoi', 'Vietnam'),\n",
       " ('Athens', 'Greece', 'Havana', 'Cuba'),\n",
       " ('Athens', 'Greece', 'Helsinki', 'Finland')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_analogy_test:\n",
    "    def __init__(self, dataset, model):\n",
    "        self.n = df.shape[0]\n",
    "        self.data = dataset\n",
    "        self.word_vectors = model.wv\n",
    "        self.vocab_w2v = list(self.word_vectors.vocab.keys())\n",
    "        \n",
    "    def Cos_Add_Mul(self, epsilon = 0.001):\n",
    "        pred_add = []\n",
    "        pred_mul = []\n",
    "        act = []\n",
    "        n = len(self.data)\n",
    "        a = 1\n",
    "        \n",
    "        for tup in self.data:\n",
    "            cosine_add = []\n",
    "            cosine_mul = []\n",
    "            if all([True for txt in tup if txt in self.vocab_w2v]):\n",
    "                candi_b_star = [voc for voc in self.vocab_w2v if voc not in tup[:3]]\n",
    "                print('start')\n",
    "                \n",
    "                for b_star in candi_b_star:\n",
    "                    first = self.word_vectors.similarity(w1=b_star, w2=tup[1])\n",
    "                    second = self.word_vectors.similarity(w1=b_star, w2=tup[0])\n",
    "                    third = self.word_vectors.similarity(w1=b_star, w2=tup[2])\n",
    "                    cosine_add.append(first - second + third)\n",
    "                    cosine_mul.append((first*third)/(second+epsilon))\n",
    "                    \n",
    "                max_idx_add = cosine_add.index(max(cosine_add))\n",
    "                max_idx_mul = cosine_mul.index(max(cosine_mul))\n",
    "                \n",
    "                pred_add.append( candi_b_star[max_idx_add] )\n",
    "                pred_mul.append( candi_b_star[max_idx_mul] )\n",
    "                act.append(tup[3])\n",
    "            \n",
    "            else: pass\n",
    "            \n",
    "            if a % 100 == 0:\n",
    "                print('전체 {0}개 중 {1}개 했음'.format(n,a))\n",
    "            \n",
    "            a+=1\n",
    "    \n",
    "            \n",
    "        acc_add = [True if i == j else False for i,j in zip(pred_add,act)]\n",
    "        acc_mul = [True if i == j else False for i,j in zip(pred_mul,act)]\n",
    "        \n",
    "        return sum(acc_add)/len(acc_add), sum(acc_mul)/len(acc_mul)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
