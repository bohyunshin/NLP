{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick View on LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input_dim: dimension of word vector\n",
    "* hidden_dim: dimension of hidden layer and cell state\n",
    "* n_layer: number of layers which is stacked on top of lstm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bath_size\n",
    "* sqe_len: length of input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "# initialize input \n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# initialize hidden state, cell state\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "\n",
    "# store hidden, cell state in tuple\n",
    "hidden = (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 1, 10])\n",
      "Hidden:  (tensor([[[ 0.0873,  0.2074,  0.3934,  0.3425,  0.1360,  0.1399,  0.0492,\n",
      "          -0.1113,  0.5139, -0.3220]]], grad_fn=<StackBackward>), tensor([[[ 0.1367,  0.3643,  0.4995,  0.7336,  0.1851,  0.5483,  0.0989,\n",
      "          -0.2053,  0.7556, -0.5570]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(\"Output shape: \", out.shape)\n",
    "print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wth sequence length 3 ( = when the number of input sentences have 3 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 3\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# if sentiment classification is the goal, take the last output of hidden layer\n",
    "out = out.squeeze()[-1, :]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon review sentiment analysis (kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sbh0613\\anaconda\\lib\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "train_file = bz2.BZ2File('./data/amazon_train.bz2')\n",
    "test_file = bz2.BZ2File('./data/amazon_test.bz2')\n",
    "\n",
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 10000\n",
    "num_test = 5000\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels from sentences\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
    "\n",
    "# Some simple cleaning of data\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dictionary that maps each word to its occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        words.update([word.lower()])  # Converting all the words to lowercase\n",
    "        train_sentences[i].append(word)\n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/num_train) + \"% done\")\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to use nn.Embedding()\n",
    "* remove words that occurr only once\n",
    "* add vocabulary 'unknown', 'padding'\n",
    "* create dictionary that maps vocab to integer and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD','_UNK'] + words\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by using word2idx, we can transform sentences consisting of natural language to index integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 1 for word in sentence]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad short sentence by filling with 0 or long sentence by shortening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 200)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     1,    87,    17,     3,\n",
       "           1,    13,    11,   221,   487,    18,   357,    15,     9,\n",
       "        6009,     3,     1,    14,    75,   409,    35,    89,     5,\n",
       "          49,  1714,     9,    87,     8,   127,    77,   717, 12277,\n",
       "           2,   158,   141,    15,     5,    27,   506,     3,   158,\n",
       "        7360,  2150,    22,    59,    10,    36,    10,     3,   626,\n",
       "           5,    27,   122,   506,     9,    60,     3,   106,   141,\n",
       "          15,     9,  9866,   271,    53,  2715,     1,     6,   402,\n",
       "           7, 12278,  1319,    24,     1,  3419,     6,  6010,     1,\n",
       "           2,     9,    49,  3267,   192,    77,  2325,     8,   383,\n",
       "          15,  8408])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_sentences[0]))\n",
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train, val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # shape of lstm_out: (50, 200, 512)\n",
    "        \n",
    "        h_t = lstm_out[:,-1,:]\n",
    "        h_t = self.dropout(h_t)\n",
    "        logit = self.fc(h_t)\n",
    "\n",
    "        return logit, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "n_classes = 2\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, n_classes, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "# criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, batch_size, print_every):\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    print('starting training...')\n",
    "    print('총 {0}개의 훈련 데이터에 대해서 훈련 시작'.format(len(train_loader.dataset.tensors[1])))\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        labels = labels.long()\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        counter += len(labels)\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            print('{0}개째 하는 중ㅎㅎ'.format(counter))\n",
    "        \n",
    "def evaluate(model, val_loader, batch_size):\n",
    "    model.eval()\n",
    "    \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for inp, lab in val_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inp, lab = inp.to(device), lab.to(device)\n",
    "        out, val_h = model(inp, val_h)\n",
    "        \n",
    "        lab = lab.long()\n",
    "        loss = F.cross_entropy(out, lab, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        corrects += (out.max(1)[1].view(lab.size()).data == lab.data).sum()\n",
    "        \n",
    "        \n",
    "    size = len(val_loader.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "if not best_val_loss or val_loss < best_val_loss:\n",
    "    if not os.path.isdir(\"snapshot\"):\n",
    "        os.makedirs(\"snapshot\")\n",
    "    torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
    "    best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "총 10000개의 훈련 데이터에 대해서 훈련 시작\n",
      "100개째 하는 중ㅎㅎ\n",
      "200개째 하는 중ㅎㅎ\n",
      "300개째 하는 중ㅎㅎ\n",
      "400개째 하는 중ㅎㅎ\n",
      "500개째 하는 중ㅎㅎ\n",
      "600개째 하는 중ㅎㅎ\n",
      "700개째 하는 중ㅎㅎ\n",
      "800개째 하는 중ㅎㅎ\n",
      "900개째 하는 중ㅎㅎ\n",
      "1000개째 하는 중ㅎㅎ\n",
      "1100개째 하는 중ㅎㅎ\n",
      "1200개째 하는 중ㅎㅎ\n",
      "1300개째 하는 중ㅎㅎ\n",
      "1400개째 하는 중ㅎㅎ\n",
      "1500개째 하는 중ㅎㅎ\n",
      "1600개째 하는 중ㅎㅎ\n",
      "1700개째 하는 중ㅎㅎ\n",
      "1800개째 하는 중ㅎㅎ\n",
      "1900개째 하는 중ㅎㅎ\n",
      "2000개째 하는 중ㅎㅎ\n",
      "2100개째 하는 중ㅎㅎ\n",
      "2200개째 하는 중ㅎㅎ\n",
      "2300개째 하는 중ㅎㅎ\n",
      "2400개째 하는 중ㅎㅎ\n",
      "2500개째 하는 중ㅎㅎ\n",
      "2600개째 하는 중ㅎㅎ\n",
      "2700개째 하는 중ㅎㅎ\n",
      "2800개째 하는 중ㅎㅎ\n",
      "2900개째 하는 중ㅎㅎ\n",
      "3000개째 하는 중ㅎㅎ\n",
      "3100개째 하는 중ㅎㅎ\n",
      "3200개째 하는 중ㅎㅎ\n",
      "3300개째 하는 중ㅎㅎ\n",
      "3400개째 하는 중ㅎㅎ\n",
      "3500개째 하는 중ㅎㅎ\n",
      "3600개째 하는 중ㅎㅎ\n",
      "3700개째 하는 중ㅎㅎ\n",
      "3800개째 하는 중ㅎㅎ\n",
      "3900개째 하는 중ㅎㅎ\n",
      "4000개째 하는 중ㅎㅎ\n",
      "4100개째 하는 중ㅎㅎ\n",
      "4200개째 하는 중ㅎㅎ\n",
      "4300개째 하는 중ㅎㅎ\n",
      "4400개째 하는 중ㅎㅎ\n",
      "4500개째 하는 중ㅎㅎ\n",
      "4600개째 하는 중ㅎㅎ\n",
      "4700개째 하는 중ㅎㅎ\n",
      "4800개째 하는 중ㅎㅎ\n",
      "4900개째 하는 중ㅎㅎ\n",
      "5000개째 하는 중ㅎㅎ\n",
      "5100개째 하는 중ㅎㅎ\n",
      "5200개째 하는 중ㅎㅎ\n",
      "5300개째 하는 중ㅎㅎ\n",
      "5400개째 하는 중ㅎㅎ\n",
      "5500개째 하는 중ㅎㅎ\n",
      "5600개째 하는 중ㅎㅎ\n",
      "5700개째 하는 중ㅎㅎ\n",
      "5800개째 하는 중ㅎㅎ\n",
      "5900개째 하는 중ㅎㅎ\n",
      "6000개째 하는 중ㅎㅎ\n",
      "6100개째 하는 중ㅎㅎ\n",
      "6200개째 하는 중ㅎㅎ\n",
      "6300개째 하는 중ㅎㅎ\n",
      "6400개째 하는 중ㅎㅎ\n",
      "6500개째 하는 중ㅎㅎ\n",
      "6600개째 하는 중ㅎㅎ\n",
      "6700개째 하는 중ㅎㅎ\n",
      "6800개째 하는 중ㅎㅎ\n",
      "6900개째 하는 중ㅎㅎ\n",
      "7000개째 하는 중ㅎㅎ\n",
      "7100개째 하는 중ㅎㅎ\n",
      "7200개째 하는 중ㅎㅎ\n",
      "7300개째 하는 중ㅎㅎ\n",
      "7400개째 하는 중ㅎㅎ\n",
      "7500개째 하는 중ㅎㅎ\n",
      "7600개째 하는 중ㅎㅎ\n",
      "7700개째 하는 중ㅎㅎ\n",
      "7800개째 하는 중ㅎㅎ\n",
      "7900개째 하는 중ㅎㅎ\n",
      "8000개째 하는 중ㅎㅎ\n",
      "8100개째 하는 중ㅎㅎ\n",
      "8200개째 하는 중ㅎㅎ\n",
      "8300개째 하는 중ㅎㅎ\n",
      "8400개째 하는 중ㅎㅎ\n",
      "8500개째 하는 중ㅎㅎ\n",
      "8600개째 하는 중ㅎㅎ\n",
      "8700개째 하는 중ㅎㅎ\n",
      "8800개째 하는 중ㅎㅎ\n",
      "8900개째 하는 중ㅎㅎ\n",
      "9000개째 하는 중ㅎㅎ\n",
      "9100개째 하는 중ㅎㅎ\n",
      "9200개째 하는 중ㅎㅎ\n",
      "9300개째 하는 중ㅎㅎ\n",
      "9400개째 하는 중ㅎㅎ\n",
      "9500개째 하는 중ㅎㅎ\n",
      "9600개째 하는 중ㅎㅎ\n",
      "9700개째 하는 중ㅎㅎ\n",
      "9800개째 하는 중ㅎㅎ\n",
      "9900개째 하는 중ㅎㅎ\n",
      "10000개째 하는 중ㅎㅎ\n",
      "[이폭: 1] 검증 오차: 0.38 | 검증 정확도:83.00\n",
      "starting training...\n",
      "총 10000개의 훈련 데이터에 대해서 훈련 시작\n",
      "100개째 하는 중ㅎㅎ\n",
      "200개째 하는 중ㅎㅎ\n",
      "300개째 하는 중ㅎㅎ\n",
      "400개째 하는 중ㅎㅎ\n",
      "500개째 하는 중ㅎㅎ\n",
      "600개째 하는 중ㅎㅎ\n",
      "700개째 하는 중ㅎㅎ\n",
      "800개째 하는 중ㅎㅎ\n",
      "900개째 하는 중ㅎㅎ\n",
      "1000개째 하는 중ㅎㅎ\n",
      "1100개째 하는 중ㅎㅎ\n",
      "1200개째 하는 중ㅎㅎ\n",
      "1300개째 하는 중ㅎㅎ\n",
      "1400개째 하는 중ㅎㅎ\n",
      "1500개째 하는 중ㅎㅎ\n",
      "1600개째 하는 중ㅎㅎ\n",
      "1700개째 하는 중ㅎㅎ\n",
      "1800개째 하는 중ㅎㅎ\n",
      "1900개째 하는 중ㅎㅎ\n",
      "2000개째 하는 중ㅎㅎ\n",
      "2100개째 하는 중ㅎㅎ\n",
      "2200개째 하는 중ㅎㅎ\n",
      "2300개째 하는 중ㅎㅎ\n",
      "2400개째 하는 중ㅎㅎ\n",
      "2500개째 하는 중ㅎㅎ\n",
      "2600개째 하는 중ㅎㅎ\n",
      "2700개째 하는 중ㅎㅎ\n",
      "2800개째 하는 중ㅎㅎ\n",
      "2900개째 하는 중ㅎㅎ\n",
      "3000개째 하는 중ㅎㅎ\n",
      "3100개째 하는 중ㅎㅎ\n",
      "3200개째 하는 중ㅎㅎ\n",
      "3300개째 하는 중ㅎㅎ\n",
      "3400개째 하는 중ㅎㅎ\n",
      "3500개째 하는 중ㅎㅎ\n",
      "3600개째 하는 중ㅎㅎ\n",
      "3700개째 하는 중ㅎㅎ\n",
      "3800개째 하는 중ㅎㅎ\n",
      "3900개째 하는 중ㅎㅎ\n",
      "4000개째 하는 중ㅎㅎ\n",
      "4100개째 하는 중ㅎㅎ\n",
      "4200개째 하는 중ㅎㅎ\n",
      "4300개째 하는 중ㅎㅎ\n",
      "4400개째 하는 중ㅎㅎ\n",
      "4500개째 하는 중ㅎㅎ\n",
      "4600개째 하는 중ㅎㅎ\n",
      "4700개째 하는 중ㅎㅎ\n",
      "4800개째 하는 중ㅎㅎ\n",
      "4900개째 하는 중ㅎㅎ\n",
      "5000개째 하는 중ㅎㅎ\n",
      "5100개째 하는 중ㅎㅎ\n",
      "5200개째 하는 중ㅎㅎ\n",
      "5300개째 하는 중ㅎㅎ\n",
      "5400개째 하는 중ㅎㅎ\n",
      "5500개째 하는 중ㅎㅎ\n",
      "5600개째 하는 중ㅎㅎ\n",
      "5700개째 하는 중ㅎㅎ\n",
      "5800개째 하는 중ㅎㅎ\n",
      "5900개째 하는 중ㅎㅎ\n",
      "6000개째 하는 중ㅎㅎ\n",
      "6100개째 하는 중ㅎㅎ\n",
      "6200개째 하는 중ㅎㅎ\n",
      "6300개째 하는 중ㅎㅎ\n",
      "6400개째 하는 중ㅎㅎ\n",
      "6500개째 하는 중ㅎㅎ\n",
      "6600개째 하는 중ㅎㅎ\n",
      "6700개째 하는 중ㅎㅎ\n",
      "6800개째 하는 중ㅎㅎ\n",
      "6900개째 하는 중ㅎㅎ\n",
      "7000개째 하는 중ㅎㅎ\n",
      "7100개째 하는 중ㅎㅎ\n",
      "7200개째 하는 중ㅎㅎ\n",
      "7300개째 하는 중ㅎㅎ\n",
      "7400개째 하는 중ㅎㅎ\n",
      "7500개째 하는 중ㅎㅎ\n",
      "7600개째 하는 중ㅎㅎ\n",
      "7700개째 하는 중ㅎㅎ\n",
      "7800개째 하는 중ㅎㅎ\n",
      "7900개째 하는 중ㅎㅎ\n",
      "8000개째 하는 중ㅎㅎ\n",
      "8100개째 하는 중ㅎㅎ\n",
      "8200개째 하는 중ㅎㅎ\n",
      "8300개째 하는 중ㅎㅎ\n",
      "8400개째 하는 중ㅎㅎ\n",
      "8500개째 하는 중ㅎㅎ\n",
      "8600개째 하는 중ㅎㅎ\n",
      "8700개째 하는 중ㅎㅎ\n",
      "8800개째 하는 중ㅎㅎ\n",
      "8900개째 하는 중ㅎㅎ\n",
      "9000개째 하는 중ㅎㅎ\n",
      "9100개째 하는 중ㅎㅎ\n",
      "9200개째 하는 중ㅎㅎ\n",
      "9300개째 하는 중ㅎㅎ\n",
      "9400개째 하는 중ㅎㅎ\n",
      "9500개째 하는 중ㅎㅎ\n",
      "9600개째 하는 중ㅎㅎ\n",
      "9700개째 하는 중ㅎㅎ\n",
      "9800개째 하는 중ㅎㅎ\n",
      "9900개째 하는 중ㅎㅎ\n",
      "10000개째 하는 중ㅎㅎ\n",
      "[이폭: 2] 검증 오차: 0.38 | 검증 정확도:83.00\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "print_every = 100\n",
    "\n",
    "# best_val_loss = None\n",
    "for e in range(1, epochs+1):\n",
    "    train(model, optimizer, train_loader, batch_size, print_every)\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, batch_size)\n",
    "\n",
    "    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
