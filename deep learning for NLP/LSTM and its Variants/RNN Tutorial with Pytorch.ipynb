{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tutorial using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* pytorch로 시작하는 딥러닝 (wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with single layer: manaully in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleRnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SingleRnn, self).__init__()\n",
    "        \n",
    "        # input_size: diimension of word vectors\n",
    "        # hidden_size: dimension of hidden layer\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wx = torch.randn(hidden_size, input_size) \n",
    "        self.Wh = torch.randn(hidden_size, hidden_size)\n",
    "        self.b = torch.randn(hidden_size, 1)\n",
    "        \n",
    "    def forward_not_loop(self, X1, X2):\n",
    "        # 원래 intput은 sequence of words인데\n",
    "        # 여기서는 두 단어로 이루어진 문장을 입력받았다고 가정한다.\n",
    "        \n",
    "        h0 = torch.zeros(hidden_size,1) # initial value of hidden state\n",
    "        \n",
    "        h1 = torch.tanh(torch.mm(self.Wh, h0) + torch.mm(self.Wx, X1) + self.b)\n",
    "        h2 = torch.tanh(torch.mm(self.Wh, h1) + torch.mm(self.Wx, X2) + self.b)\n",
    "        return h1, h2\n",
    "    \n",
    "    def forward(self, seq_inputs):\n",
    "        \n",
    "        # seq_inputs은 num_words * dim_embedding의 matrix라고 가정.\n",
    "        \n",
    "        total_hidden_states = []\n",
    "        h0 = torch.zeros(self.hidden_size,1)\n",
    "        \n",
    "        for i in range(seq_inputs.shape[0]):\n",
    "            x = seq_inputs[i,:].reshape(self.input_size, 1)\n",
    "            h1 = torch.tanh(torch.mm(self.Wh, h0) + torch.mm(self.Wx, x) + self.b)\n",
    "            total_hidden_states.append(h1)\n",
    "            h0 = h1\n",
    "        return torch.stack(total_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 128, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = SingleRnn(input_size=100, hidden_size=128)\n",
    "word_vectors = torch.randn(300,100) # total 300 words and 100 dimension\n",
    "result = rnn.forward(word_vectors)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with single layer: using pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, inputs, word_size, hidden_size):\n",
    "        # word_size: dimension of word vector\n",
    "        # hidden_size: dimensino of hidden state\n",
    "        # inputs: (batch_size, time_steps, word_size): (1,10,5)는 배치 크기는 1, 10번의 시점동안 5차원의 벡터가 들어감.\n",
    "        # batch_first: 첫 번째 차원이 배치 크기임을 알려준다.\n",
    "        \n",
    "        super(SimpleRNN, self).__init__()\n",
    "        cell = nn.RNN(word_size, hidden_size, batch_first=True)\n",
    "        self.outputs, self.final_hidden = cell(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 8]), torch.Size([1, 1, 8]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = SimpleRNN(torch.Tensor(1,10,5), 5, 8)\n",
    "# nn.RNN은 두 개의 값을 반환한다.\n",
    "# 첫 번째는 전체 time steps에서의 hidden state랑 마지막의 hidden state 값이다.\n",
    "rnn.outputs.shape, rnn.final_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0930, -0.3091,  0.6173, -0.2054, -0.2841, -0.0620,  0.1653,  0.5056],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.outputs[0,9,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0930, -0.3091,  0.6173, -0.2054, -0.2841, -0.0620,  0.1653,\n",
       "           0.5056]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.final_hidden\n",
    "# 둘의 output이 같은 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with multi layers: using pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRNN(nn.Module):\n",
    "    def __init__(self, inputs, word_size, hidden_size, n_layers):\n",
    "        # word_size: dimension of word vector\n",
    "        # hidden_size: dimensino of hidden state\n",
    "        # inputs: (batch_size, time_steps, word_size): (1,10,5)는 배치 크기는 1, 10번의 시점동안 5차원의 벡터가 들어감.\n",
    "        # batch_first: 첫 번째 차원이 배치 크기임을 알려준다.\n",
    "        # multilayer rnn은 num_layers 인자만 설정해주면 된다.\n",
    "        \n",
    "        super(MultiRNN, self).__init__()\n",
    "        cell = nn.RNN(word_size, hidden_size, batch_first=True, num_layers = n_layers)\n",
    "        self.outputs, self.final_hidden = cell(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 8]), torch.Size([2, 1, 8]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = MultiRNN(torch.Tensor(1,10,5), word_size=5, hidden_size=8, n_layers=2)\n",
    "# multilayers일 때는 (층의 개수, 배치 크기, hidden state의 dim)\n",
    "rnn.outputs.shape, rnn.final_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0105, -0.4557, -0.3854,  0.4029,  0.2252, -0.1533, -0.2856,  0.0045],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.outputs[0,9,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6744,  0.1579, -0.4215,  0.8734, -0.0214,  0.3211,  0.8059,\n",
       "           0.4949]],\n",
       "\n",
       "        [[ 0.0105, -0.4557, -0.3854,  0.4029,  0.2252, -0.1533, -0.2856,\n",
       "           0.0045]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.final_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN with multi layers: using pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, inputs, word_size, hidden_size, n_layers):\n",
    "        # word_size: dimension of word vector\n",
    "        # hidden_size: dimensino of hidden state\n",
    "        # inputs: (batch_size, time_steps, word_size): (1,10,5)는 배치 크기는 1, 10번의 시점동안 5차원의 벡터가 들어감.\n",
    "        # batch_first: 첫 번째 차원이 배치 크기임을 알려준다.\n",
    "        # multilayer rnn은 num_layers 인자만 설정해주면 된다.\n",
    "        \n",
    "        super(BiRNN, self).__init__()\n",
    "        cell = nn.RNN(word_size, hidden_size, batch_first=True, num_layers = n_layers, bidirectional=True)\n",
    "        self.outputs, self.final_hidden = cell(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 16]), torch.Size([4, 1, 8]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = BiRNN(torch.Tensor(1,10,5), word_size=5, hidden_size=8, n_layers=2)\n",
    "# multilayers일 때는 (층의 개수, 배치 크기, hidden state의 dim)\n",
    "rnn.outputs.shape, rnn.final_hidden.shape\n",
    "\n",
    "# 전체 hidden state의 갯수가 8개에서 16개로 2배만큼 늘어남."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
